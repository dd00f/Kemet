

2018-11-08
	RandomAI created :
		It just picks random choices. Dumb, but good for randomized testing.

2018-11-16
	TrialAI created : 
		It tries every choice for the next action and picks the best option, a bit smarter.
		Created a scoring algorithm that evaluates the position of a player as a single number.
	
2018-11-19
	TrialAI improved : 
		It now looks at all actions until another player needs to act. Getting a bit smarter.
		
	TrialAI improved : 
		It now favors defending pyramids over empty territories.
	
2018-11-23
	TrialAI improved : 
		It now looks at all actions until the end of the turn. 
		Assuming opponents always picks the best option for them.
		
	TrialAI improved : 
		it now compares its score with opponents, favoring moves that puts it ahead. 
		
2018-11-25
	TrialAI found a bug : 
		When the attacker won, but got destroyed, and there are no recall option for the defender, 
		recall was forced even though the defender could stay where they were. Good boy.
	
	TrialAI is cheating when defending : 
		It knows which card the attacker picked as if it had prescience.
		On attack, it assumes the defender will always pick the best card, a bit of a disadvantage.
		There's also no logic that hides cards that were discarded.
		Gotta fix that eventually.

2018-11-27
	TrialAI defender no longer sees attacker card.
		It now assumes the attacker will pick the best card it can, no matter what it chooses, 
		quite a pessimistic view of things.

	TrialAI averages battle possibilities.
		It now assumes the opponent can't see what it will pick. So it takes the average score of picking each
		battle card, leading to the most logical, and predictable choice.

	TrialAI assumes discarded battlecards can be used.
		It's hidden information, so TrialAI no longer knows which cards were discarded, 
		It just assumes all cards are available.

2018-11-29
	TrialAI isn't very good at recruiting 
		Since any army score is worth more than raw prayer points. It recruits puny armies.
		To fix this, TrialAI will need to try to simulate a few more actions.

2018-12-22
	deepCacheClone is fast !
		Implemented the first implementation of deepCacheClone which skips serialization of game state for 
		simulations and reuses a cache of all objects that were created in previous simulations.
		While it doesn't reuse all objects( choices are still created every time from the heap), the 
		simulation speed took a meaningful jump from 6,000 simulated choices per second to 160,000 simulated
		choices per second. 26.6x faster !
		
		But somehow, the AI seems dumber. It picks really odd battle cards. Might be because it tries to pick 
		many choices in a single simulation. 
		
		But gawd is it painful to track all object creation and release like I did in the good old C++ days.
		
2018-12-23
	Such an optimist AI
		Turns out the simulation caching AI uses the best possible battle outcome to pick its choice, regardless
		of the odds. Time to ignore the score past the points where we can't guarantee the outcome.
		
	Caching Smaching
		Hmmm... I neutered the cache and now the JVM creates a ton and a half of objects instead of reusing them.
		Turns out... the cache doesn't change anything in performance, performance even improved a bit from 
		161,000 simulation per second to 165,000. Damn... that heap allocation &  garbage collection got better over 
		time. I can even multithread now !
		
		Moral of the story : caching objects in Java is pointless, serialization is ~25x slower than straight copy.

2018-12-29
	Multi-threading is easy.
		Made the simulation multi threaded by using java streams. My 4 core CPU went from 165,000 simulation per second
		to 680,000 simulation per second. 4.1x faster ! Speed even reached 769,000 simulation per second in the 
		later stages, suspect that limiting thread count will help here. Now it only takes 18 minutes for the AI to 
		pick its initial pyramids after only 568 million simulations... woohoo !

2018-12-30
	Monte carlo gambling.
		Reading on alpha go zero :
		https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ
		quote : 
			The neural network consists of many residual blocks of convolutional layers 
			with batch normalization and rectifier nonlinearities (see Methods).

		Easy, right ?
		
		So, a few difficult challenge ahead : 
		1 - Map the board state to a matrix of bytes or floats
			- how do I do that with some stuff that's variable like the number of armies ? I guess I'll just
			  assume a maximum number of armies per side to simplify things.
			
		2 - Map a neural network that will calculate a board value.
			How many layers will it need ? What kind of layers ?
			
		3 - Map a neural network that will pick the best move based on board value ?
			Has to start by assigning the same priority to every move so that they get randomly picked.
			
		4 - How is feedback sent back to the network ?
			Games are fully replayable, so we could remember the full sequence of actions, apply a boost to the winner,
			apply a negative to the loser.
			
			- start with ... X layers of blank
			- but multiple layers, they only make sense when they are somewhat randomized, otherwise, 
			  where do you you apply boosts on randomized math equations ?
			- by looking at how the board changed between moves ? 
			- Good, but what about changes that occur because of past actions ? AKA, get prayer bonus due to temple,
			  but those would just keep on getting boosted.
		
		 Found a good example of the AlphaGo Zero methodology in python here : 
		 https://web.stanford.edu/~surag/posts/alphazero.html
		 https://github.com/suragnair/alpha-zero-general
		 
		 Should be translateable to my java model.
		 
		 Vector of all possible actions : 61 possible choices
		 	- end action
		 		1 choice
		 	- army size pick
		 		7 choices : 1 to 7
		 	- pick tile
		 		13 choices : 2 player game has 13 tiles
		 	- pick pyramid color
		 		4 choices : 4 color
		 	- pick pyramid level
		 		4 choices
		 	- pick army
		 		13 choices : 1 army per tile
		 	- pick battle card
		 		8 choices : 1 per card
		 	- recall army
		 	    1 choice
		 	- player token pick
		 		10 choices - for each token

		
		Vector of game state: 200 datapoints
			for game : 37 datapoints
				public byte roundNumber = 0;
				One per state : 18 datapoints ( 1 for current player, -1 for oponent player, 0 for inactive state )
					- recruit state
					- move state
					- battle state
					- upgrade pyramid state
					- pick action token
					- pick attack battle card
					- pick attack discard
					- pick defense battle card
					- pick defense discard
					- pick army size
					- pick tile
					- pick pyramid color
					- pick pyramid level
					- pick army
					- pick attacker recall
					- pick defender recall
					- pick attacker retreat
					- pick defender retreat

				One per ongoing selected data : 7 datapoints 
					- 6 datapoints : battle attack/defense strength/shield/damage
					- picked size
				
				Battle stats to help nudge things ? 2x2x3 = 12 datapoints
					- min/max attack/defense strength/shield/damage
	
			for each power : 0 x 1 = 0 datapoint
				owning player index

			for each tile : 13 X 9 =  117 datapoints
				2 datapoints : army strength per player
				is picked
				2 datapoints : pyramid level per player
				4 datapoints : is pyramid of color X
				
			
			for each player : 23 datapoints X 2 = 46 datapoints
				public byte victoryPoints = 0;
				public byte battlePoints = 0;
				private byte prayerPoints = 5;
				public byte availableArmyTokens = 12;			
				public boolean rowOneMoveUsed = false;
				public boolean rowOneRecruitUsed = false;
				public boolean rowTwoMoveUsed = false;
				public boolean rowTwoUpgradePyramidUsed = false;
				public boolean rowTwoPrayUsed = false;
				public boolean rowThreePrayUsed = false;
				public boolean rowThreeBuildWhiteUsed = false;
				public boolean rowThreeBuildRedUsed = false;
				public boolean rowThreeBuildBlueUsed = false;
				public boolean rowThreeBuildBlackUsed = false;
				public byte actionTokenLeft = 5;
				for each battle card : is available ( 8 datapoints )


2019-01-01
	Canonical form
		The whole game state and choice list is now canonical & hopefully ready for neural network training. Happy new year !

2019-01-03
	Monte-Carlo simulation
		Recoding MTSC in Java really helps to understand how this thing really works. Left todo : Neural Net & Coach.
	
2019-02-10
	Neural network is plugged in !
		It's probably full of bugs, but at least it compiles. Took longer than expected to learn ND4J, Neural Network, 
		regression functions, why MTSC works, TD Lambda, all that stuff.

2019-02-12
	Neural network is a dumbass
		First round of bugs fixed. The neural network can now play a full game without crashing, but boy is it dumb.
		Takes about 28 seconds to play a full game with 25 MCTS trial per move. Need more stats about where time is spent.
		Had to limit the game to 20 turns otherwise the AI sometimes just twiddled its thumbs forever.
		

2019-02-18
	Good neural network, here's a candy
		There are so many things that causes a neural network to collapse over training. Layers too small, layers too big, 
		too many layers, not enough layers, wrong loss function, wrong activation function, wrong learning value.
		Ran a mini test with 10 inputs 10 policy output. turns out 6 dense layers at 60 neurons worked best. Recurrent layers didn't add much.
		Time to try it at the simple kemet size.
		
		Later that day : Good news bad news 
		
		The good - We have our first iteration of a Neural Net that actually learned something after 2 hours ! Building level 4 pyramids = good !
		
		The bad news - It takes a while to learn. 
		
			- per game
				- 30 moves per MTCS search
				- 3.6ms per Neural Network predict calls
				- 12000 Neural Network predict calls per game
				- time per game 45 seconds ( should get lower once the AI can reach 8 points to end the game faster )
				- 10 games learning
				- 8 minutes to play games
			
			- per training
				- 10 epoch
				- 20,000 moves to train
				- 8m45s to train one epoch
				- 1h17m30s to train one arena
			 
			 - Coach cycle : 
			 	- 8 minutes to self play 10 games
			 	- 1 hour to train a new neural net
			 	- 8 minutes to pit the old vs the new in the arena
			 	
			 	- 1h16m per coach cycle.
				
		
		Lets say that we always learn at 10 games per hour if everything is perfect. 
		- 10 game per hour
		- 240 game per day
		- 1680 game per week, doesn't sound like much
		
		Ugh... and they trained AlphaGoZero in 40 days.
		
			
		GPU-CUDA based implementations do not seem to make much of a change to this. I guess CPUs are pretty good at large matrix
		calculations.
		
		Also, comparing a few stats from the game of go or chess : 
		
		Average number of moves per game : 
			- Chess 40
			- Go 40
			- Mini Kemet 400 micro decisions ( a single army move is 4 decision : action space, source tile, size to move, destination tile )
		
		Input vector
			- Chess 1152 ( 8*8*18 )
			- Go : 6137 ( 19*19*17 )
			- Mini-Kemet : 150
		
		Output vector
			- Chess 1152 ( 8*8*18 )
			- Go : 381 ( 19*19 )
			- Mini-Kemet : 90
		
		The biggest problem is the number of moves per game. That may mean I'll need 10x more training than other games. 
		I could merge decisions to create fewer training steps : 
		- upgrade pyramid : 6 tiles x 4 color x 4 level = 96 output
		- moves : 2 move tile x 13 tiles source * 7 army size * 5 connections per tile + 1 end turn = 911 output
			- army strength could be a fraction instead of a fixed output
		- recruit : 13 tiles * 7 army size = 91 output
		- purchase tiles " 16*4 = 64 output
		
		So, my output vector would grow from 90 to 1100, my moves per game would shrink from 400 down to ~120. It also means my MCTS 
		efficiency would triple as it wouldn't need to evaluate so many sub-states to find good values. 
		
		Next step : run performance test : 
		- Is it better to have an output that's 90 and run 3000 training fit, or an output that's 1100 and run 1000 training fit ?
		 

2019-02-20
	Batch me up Scotty
		My initial code trials set the training batch size to ... 1, you know, just to see if the code compiles and runs.
		A few copy-paste later, and my entire code base kept using a training batch size of 1.
		How much difference does the batch size make for training ?
		A lot it turns out, training is now 100 times faster, turning 2 hours to 2 minutes. Now we're talking !

2019-02-23
	Network collapse ?
		My AI training leads to an endless stream of games where the preferred action is "skip turn". Something tells me the 
		network has collapsed. A few things to try : 
		- eliminate -1 values
		- run some tests against the neural network with sample board values to see if it converges.
			- turns out that training the network with masks doesn't work.

	Recurrent network
		I created a first version of the recurrent neural network where every layer created feeds to the layer below them 
		so that lower layers never forget the initial state.
		
2019-02-24
	Bug fix
		MCTS always stored the board state according to player zero. Leading to a bug on hidden card selection state.
		
		The state normalization wasn't working. Fixed that.
		
		
2019-02-26
	Slowly getting better
		With the latest round of fixes, the Neural Network is finally getting stable. 
		After 90 iterations, the Neural Network keeps on improving. 
		- It captured experience from 3,600 games at this point.
		- On average, games now end on turn 10 instead of turn 15 because the AI gains much more 
		victory points through battle. 
		- It seems to have completely ditched the idea of building high level pyramids.
		- It doesn't seem to fully grasp the concept of temple to gain victory points, though it does show a tendency to 
		go there.
		- It doesn't get the concept of army size. It prefers to recruit many small armies and send them 
		all over the map, regardless of how likely they will win battles. Odds are, it would get
		crushed by a human opponent who would always attack with armies of size 5 and disband any weak armies.
		
	Current concerns
		
		- The neural network may struggle to fully understand state data that mixes binary states (current phase move value is 1.0 )
		with scalar data (army strength is 0.1 ). One option would be to create a binary state for scalar values :
			- instead of prayer points = 0.5
			- prayer point 1 = 1
			- prayer point 2 = 1
			- prayer point 3 = 0
			- prayer point 4 = 0
			
			- instead of army size on tile X = 0.2
			- tile X army token 1 = 1
			- tile X army token 2 = 0
			- tile X army token 3 = 0
			- tile X army token 4 = 0
			- tile X army token 5 = 0
		
		- The neural network may struggle to train with states it hasn't seen much. If it rarely sees a size 5 army, 
		it may start ignoring the option to move all 5 soldiers in an army because the MCTS zeroed out this option 
		as it wasn't available, slowly reducing the probability that this option will ever get picked. I need to figure out
		if masking outputs will fix this.
		
		- The MCTS struggles with effectiveness : with 50 search depth, it barely has time to explore the ramifications of a 
		single move action. It can only rely on the instinct it built using the value function estimation to pick best paths.
		Expanding the output tree to reduce the number of decisions may help.
		
		- The neural network may be too shallow : It's currently using about 100 features per layer, 5 layers, pyramidal 
		recurrency. That may not be enough breath and depth of knowledge to fully handle the game.
		
		- There is a bit of gambling in battle card selection, may need to apply a state learning algorithm that predicts
		how often an enemy will pick a specific card.
		
		- Move & battle execution needs to be learned in every tile ?
		The board value function must learn what it means to have an army in each different tile and how to move to that 
		specific tile. I already created features that describe the current battle, that may help...
		
		- Performance : if I read it right, AlphaGo was able to train by trying 1600 moves in each MCTS simulation in 0.004 seconds.
		My current neural network takes 0.000,666 second to run a single evaluation and it represents 99%+ of the time
		spent while running a game. At that speed, 1600 move simluation per MCTS would take 1.065 seconds. 
		That's 266 times slower than Alpha Go. Pair that with the fact that a game of Kemet takes about 400 decisions versus
		40 decisions in chess and my implementation is now 2,660 times slower than Alpha Go. 
		Translation  :
			- It took AlphaGo 1 day to reach a "good" play level.
			- It will take 7 years for AlphaKemet to reach good play level.
			
			- It took AlphaGo 7 days to reach Master level.
			- It will take 51 years for AlphaKemet to reach Master play level.
		
		On the other hand, Mini-Kemet is likely easier to solve than Chess due to its simplicity. 
		
		
	Next Steps :
	
		- Play a game vs the neural network with both a human and the trial AI. Who knows, maybe that rampaging neural network
		really found the best solution to the current subset of Kemet. 
	
		- Try to run training with masks in a way that doesn't collapse the neural network to ensure actions that aren't 
		available frequently don't become erased in policy vectors. One option : Preserve neural network policy output 
		values, only modify the ones that were actually available for trial.
		
		- Performance : Batch neural net evaluation during MCTS to try all moves in the current decision tree ?
		
		- Reduce the number of decisions to take in a game by merging them
			- Upgrade pyramid picks tile, color & level
			- Move army picks source, destination & army size
			- Combat picks both attack & discard.
			- recruit picks tile & army size
			- buying tiles & selecting the tile to buy. ( that's required anyway )
			
			- While the game remains simple, this works ok, but I wonder how it will impact things like : 
				- DI card selection during combat
					- This has to remain a separate action.
				
				- Beast modeling in terms of
					- move options would include a binary option to move beast or not. 
		 
	Version 1.0 is good to go !
		

2019-02-27

	First Human Trial
		First human vs neural network challenge game. Results : 8-0 win for camp human.
	
		Turns out this neural network isn't very smart yet, probably due to bugs in the way I trained it. 
		It seems to favor actions that are showing up frequently, since it's more likely to see small armies than 
		large ones, it favors them. Leading to an easy victory for camp humans by attacking those tiny armies.
		Since the skip turn action also shows up frequently, it seems to like that option as well.
	

2019-03-02

	Batch me up Scotty - Training mode
		When I learned that batched calls to the training fit function were over 100 times faster than individual calls, 
		I wondered if batching calls to the output function would yield the same result.
		
		A quick unit test later, I discovered that batching calls to output was 
		- 20 times faster in batches of 100 vs 1 
		- 40 times faster in batches of 1000 vs 1
		
		I could leverage this during self-plays & arena fights.
		
		This required a significant refactoring of the MCTS algorithm. It would have to run all games in parallel. 
		It couldn't be recursive anymore as	I'd have to run the search process until a neural network call is required, 
		stack all the request, call the neural network in batch, and resume the search process where it left off.
		
		Once it was done, the results were : 
		- 1 game, 10 MCTS search : 
			- Original : 3.3s
			- Parallel : 9.0s  ( 2.7x slower )
		
		- 50 game, 10 MCTS search : 
			- Original : 151s
			- Parallel :  25s  ( 6x faster ! )
		
		- 100 game, 10 MCTS search : 
			- Original : 300s
			- Parallel :  38s  ( 7.8x faster ! )
		
		Almost 8x faster to do the exact same job !  I'll drink to that !

2019-03-03
	
	Mass brawl
		The arena now runs all its games in parallel, let the games begin !
		

2019-03-05

	First training
		After 9 hours of training, the neural net now plays somewhat ok. It still loses to human opponents.
		It now moves armies in size of 5 on occasion, but still skips its turn way too often and doesn't seem
		to understand when to attack. This would probably need a deeper neural network and/or more self training.

TODO : 
		- Try to run training with masks in a way that doesn't collapse the neural network to ensure actions that aren't 
		available frequently don't become erased in policy vectors. One option : Preserve neural network policy output 
		values, only modify the ones that were actually available for trial.
		
		- make each action token a variable ? 
		
		- move action to isolated island - avoid choice if the army can't move.
		- last discard action isn't a choice, force it.
		- army retreat may have only one move, force it.
		- there may be a forced action to pick when selecting the action ( one choice left in row 3 ), force it.
		- Bug : moving army from Blue District 3 doesn't offer teleportation options.
		
		- allocate new action index for various purpose ( end turn in move, vs recruit, vs buy, etc..., army size in recruit vs move )
		
		- add all base game powers

